\documentclass[twoside,11pt]{article}

% !!!!! As a rule, use past tense to describe events that have happened. Such events include procedures that you have conducted and results that you observed. Use present tense to describe generally accepted facts. !!!!!

% Any additional packages needed should be included after jmlr2e.
% Note that jmlr2e.sty includes epsfig, amssymb, natbib and graphicx,
% and defines many common macros, such as 'proof' and 'example'.
%
% It also sets the bibliographystyle to plainnat; for more information on
% natbib citation styles, see the natbib documentation, a copy of which
% is archived at http://www.jmlr.org/format/natbib.pdf

\usepackage{jmlr2e}
\usepackage{amssymb} %maths
\usepackage{amsmath} %maths
\usepackage{multirow}
\usepackage{rotating}
\usepackage{url}

\newcommand{\dataset}{{\cal D}}
\newcommand{\fracpartial}[2]{\frac{\partial #1}{\partial  #2}}

\ShortHeadings{Location Classification on Tweets}{Erdi \c{C}all{\i}}
\firstpageno{1}

\begin{document}

\title{Location Classification on Tweets}
\author{\name Erdi \c{C}all{\i}  \email e.calli@student.ru.nl}
\maketitle
\begin{abstract}

We extracted textual features that hint to a user's location by comparing different methods. Unlike some of the previous studies, we have only used individual tweet texts, geolocation of those tweets and simplified the question to a binary classification problem. Our results are showing up to 74\% recall rates.

% Previous research on Twitter user location classification is mostly focusing on multi-class prediction. Approaching at the same problem from another perspective, in some cases, it may be enough to answer if someone belongs to a location or not. In this perspective, we have compared some models for predicting tweet locations using Binary and Multi-Class Classification. We have seen that if the problem at hand is reducible to Binary Classification, it is possible to make much better predictions.

\end{abstract}


\section{Introduction}

Twitter is an online platform that lets users to share posts with their followers, limited with at most 140 characters, called as tweets. Tweets can be almost anything that we share on internet; a personal message stating our views, a link to an article, a picture and etc.. There are no boundaries except the character limit. In 2009, Twitter introduced an option to include geographic location(geolocation) information on tweets\footnote{\url{https://blog.twitter.com/2009/location-location-location}}. Since then, a lot of twitter users have been sharing their geolocation with their tweets(or geotagging their tweets). Depending on the context of the tweet, having the geolocation may be very valuable. For example a tweet about a Pizza franchise or a political campaign, if geotagged, may point to an improvement with exact location or contribute to the statistics of a specific area. Unfortunately, geolocation of a tweet is not shared by default or in other words, most of the tweets are not geotagged, unless explicitly specified. Inferring the geolocation of a tweet is what this paper is about.

To be able to infer this information, we can try to use linguistic habits shared by the people who are within the same location. As humans, our spatial cognition deals with updating and managing the knowledge about our environment. Spatial cognition changes when we move between environments, but as we stay within the boundaries of a high level environment(i.e. state, city, borough), there may be some high-level constants that doesn't change. If some of these constants are shared commonly between people, it may lead them to have similar linguistic habits, in this case, textual features in their tweets. If we group the geotagged tweets sent within a high-level location, it may be possible to extract these features, if they exist.

Using similar features, previous research tries to determine the geolocation of a tweet using multi-class classification algorithms. These methods are trying to determine the most likely geolocation for a tweet from a list(e.g. cities in a country). It is sometimes hard to do that, because we need data from all cities. Also the algorithms and the implementations become more complicated. Thus, in cases where we just want to know if someone is within a state/city/borough, this approach becomes unnecessarily heavy. In this research, we are simplifying this problem by using binary classification algorithms, which are trying to determine if a tweet sent within the boundaries of a location(i.e. Manhattan, NY). 

Revolving around our hypothesis, the research question we're trying to answer is: "Are there textual features in tweets, that are common for people within the same location? If there are, can they help us determine if a tweet is sent within that high-level location?"


\section{Related Work}

Similar to our work, using textual features in tweets and geotags, Cheng et al.approached this question as a regression problem\cite{cheng2010you}. They have located users within a 100 mile radius and achieved up to 51\% accuracy. To do that, they used only the textual features and introduced a method that identifies these features(i.e. unique words) with strong local geo-scope.

Hecht et al. introduced the $CALGARI$ classification algorithm specific to this problem, focusing on a higher granularity of country and state prediction\cite{hecht2011tweets}. They got a 72\% accuracy for classifying 4 countries and 30\% accuracy on classifying 18 states. Their method was depending on the location information revealed by users in their tweets, and classifying user's location based on their past tweets.

Eisenstein et al. created a model using textual features in different topics(e.g. Sports, Entertainment)\cite{eisenstein2010latent}. Their research got a median distance error of 494 km for individual tweets.

Wing et al. created another model by combining geotagged Wikipedia articles and tweet texts\cite{wing2011simple}. They have created geodesic grids varying in size, and they have obtained a median error of 479 km for individual tweets. 

Mahmud et al. introduced hierarchical classification methods combined with statistical methods\cite{mahmud2012tweet}. They have used tweet texts as well as tweeting behaviour to achieve 58\%, 66\% and 78\% accuracy on city, state and timezone levels, respectively.  Same researchers revisited the problem in, trying to classify the home location of users, achieving similar results and also introducing movement and location prediction\cite{mahmud2014home}. 

Also, Graham et al. studied the problems with the baselines accepted by recent studies and explained why location and language classification is not an easy problem and how some of the assumptions are incorrect, such as accepting the device location as user's location\cite{graham2014world}. 

\section{Methods}

\subsection{Data Collection and Filtering}

Using Twitter's sample status streaming api\footnote{https://dev.twitter.com/streaming/reference/get/statuses/sample}, we have sampled the public tweets. This api samples approximately 1\% of all the public tweets around the world. To be able to find the tweets that are relevant to our task, we have applied some filters to this api. 

First of all, we have applied a filter to ignore non-geotagged tweets. The high-level location information contained within each geotag will be used as the label of a tweet. These labels will be used to train our algorithms, and they will be compared with the predicted results to see how accurate our methods are.

Also, we are not interested with tweets in different languages. Having tweets from different languages would introduce a complicated variable and make our results hard to interpret. Therefore, we choose English as our language of interest, because we assumed that English is the most common language that people tweet, thus it would yield the biggest dataset(compared to other languages). Samples from twitter contain the language information, we have used this information to filter tweets which are not English. 

Considering the language assumption, having tweets from non-English speaking countries would be unnecessary. Also, having tweets from different countries would introduce another variable. Assuming that United States would yield the biggest dataset(compared to other countries), we have limited the tweets we collected to United States.

Running the data collector for 1 month, we have collected a dataset consisting of 33112 tweets, geotagged within United States, tagged as English and each from a unique user.

Assuming we had a dataset, big enough for the task, we examined a small sample of it. In this sample, we have seen many examples which were not composed by humans, but bots(e.g. "\textit{Police department activity on \#I95 SB at 3rd Avenue; Exit 3}"). Twitter also gives the information on which application is used to send a tweet. We have examined this information, called \textit{source}, to filter these tweets. Grouping and sorting them by the number of tweets, we were able to see which sources are most relevant for our study. Annotating the most relevant 20(e.g. \textit{Twitter for iPhone}, \textit{Facebook} and etc.) we have created the whitelist filter for tweet sources.

In our examination, we have also seen a variety of geotag names which represent the high-level locations we're interested in(e.g. \textit{Mississippi, USA}, \textit{Manhattan, NY} and etc.). When we checked the number of tweets per location, we have seen that there are 3306 unique locations in the whole dataset with different granularities(e.g. state, city, borough and etc). Upon examining these locations, we have seen that it was not worth merging these locations with small granularities(e.g. boroughs) to higher levels(e.g. cities). Also we are not interested in location definitions higher than cities. Therefore, we have picked 8 of the most common locations(e.g. \textit{Manhattan, NY}, \textit{Los Angeles, CA}, \textit{Houston, TX} and etc.) ignoring states. 

When we applied all these filters, we ended up having a dataset containing 8 locations, each of them having between 976 to 212 tweets, summing up to 3176.

Since each tweet is limited by 140 characters, and our dataset is shrank to only 4751 tweets, our dataset might not be enough to extract the textual features we are interested in. To enrich our database, we assumed that our users are always sending tweets from the same location. Therefore, we have collected past 40 tweets of these users and attached them to the sampled tweet as \textit{past tweets}.

In all our experiments, we have splitted our datasets 0.5 by 0.5 training and test datasets.
\subsection{Tools}
sklearn python mongo



\subsection{Empirical Bayes}

In the process of filtering data, we have implemented an Empirical Bayes\cite{carlin1997bayes} classifier using MAP estimation\cite{degrooth} to perform multi-class classification. We chose Empirical Bayes because it was easy to implement and it was not assuming any independency between data points, which was required when using past tweets, which may be dependent to each other. Implementing this algorithm also helped us to understand the nature of data, and also we somehow tried to reproduce results similar to previous research.
\begin{equation*}
\begin{split}
\mathbf{T}_i & = \text{tokens of $i$th tweet }\\
t_{ni} & = \text{$n$th token of $i$th tweet }\\
L &= Location \\
p(L|\mathbf{T}_i) &= \frac{p(\mathbf{T}_i | L)p(L)}{p(\mathbf{T}_{i})} 
\end{split}
\end{equation*}
To train this algorithm, we created a likelihood($p(t | L)$) and an empirical prior. To calculate the likelihood, we have tokenized each tweet using the $twokenize$ module\cite{gimpel2011part}, labelled them with the location and calculated the "probability of a token given location". As for the prior we have generated the empirical prior, which is the occurrences of a location divided by the number of data points. As a result of training, we have stored the likelihood and prior.

To predict where a tweet has been sent from we have applied MAP estimate to the Empirical Bayes. For each tweet in our test dataset, we have tokenized the tweet, queried the likelihood values for each token and location, combined these probabilities per location, applied the empirical prior and choose the location with the maximum probability.
\begin{equation*}
\hat{L} = \arg\max_L\{p(\mathbf{T}_i | L) p(L)\}\\
\end{equation*}
\begin{equation*}
p(\mathbf{T_i} | L) = \prod_n{p(t_{ni}|L)}
\end{equation*}

To test our assumption about past tweets, this experiment was ran with 2 configurations one using past tweets and other using sampled tweets. In the first configuration, we combined each past tweet to one document and labelled them with the high-level location of the related sample(tweet). We have used this configuration to test our assumption of users are always sending tweets within same high-level location. In the second configuration, we only used the sample tweets and their high-level locations as labels.

First run of the experiment, without applying filters to the dataset, resulted with 0.02 and 0.04 macro-average precision rates for the first and second configurations, respectively. After this experiment, we have seen that the assumption about past tweets did not improve our results. The rest of this Empirical Bayes experiment was ran only with the second configuration, sampled tweets.

One problem with this experiment was, if a token($t_x$) didn't appear in the training dataset, we didn't know the likelihood of that token. So if there is a token did not appear in the training, the tweets containing that token got a probability of 0 for all locations, regardless of the other tokens. To solve this problem, we have ignored those tokens in our calculations. This solution, is not mathematically correct because ignoring a token means setting it's probability to 1. Still, replacing those probabilities improved the macro-average precision to 0.05.

When we applied the location whitelist, we have achieved a macro-average precision of 0.11. Which doesn't seem better than making random classifications for 8 locations or picking the location which occurred the most. 

So our experiment with Empirical Bayes classifier failed to produce results better than any baseline, but it helped us understand the nature of data and what kind of filters we need to apply.

\subsection{Scikit Learn - Experiment Setup}
After failing to produce results better than making random choices, we decided to use the Scikit Learn \cite{scikit-learn}(sklearn, v0.17) library to actually test our hypothesis using binary classification and compare our results with multi-class classification.

In this set of experiments, we have re-define our experiment setting. Since we started using a library, we have access to more complicated tools which helps us to define better experiment settings. This experiment uses sparse arrays to represent tweets. For each tweet, there is a 1 dimensional sparse array with columns representing tokens(in our dataset). Combining these, we have created two 2 dimensional sparse arrays, representing our training and test datasets. As for the values representing tokens, we have used tf-idf scores. 

We chose tf-idf scores because we wanted to reduce the importance of stopwords and frequently used words. This experiment learns idf scores from the training dataset, and uses these scores to calculate the tf-idf scores for training and test datasets. 

Using tf-idf scores and sparse arrays, we can't capture textual features created by a sequence of words. A good example to that would be \textit{The Empire State}. \textit{The}, \textit{Empire} and \textit{State} are words that could be used without a spatial context. Our sparse array would only contain individual information about the tf-idf scores of these tokens. So we are losing the information of using them together, which may be a textual feature, pointing to Manhattan, New York.

To overcome this limitation, we included word ngram tf-idf scores in our sparse arrays. Doing so we got information about word sequences, which would possibly solve the problem. We have experimented with 2 configurations, first one using only the tokens or word unigrams, second one using  word unigrams with word bigrams. 

We also tried to make use of past tweets by labelling them individually with the location of their sample tweet. We created 2 configurations for this, one using the sample tweets, other using the sample tweets.

Putting it all together, we had 8 different configurations, combining the word ngrams, past/sample tweets and binary/multiclass classification. Again using the 0.5 by 0.5 split for train and test datasets.

\subsubsection{Multinomial Naive Bayes}

To test our experiment setup, we chose MultinomialNB(Multinomial Naive Bayes) implementation for multi-class classification. MultinomialNB considers the 

This implementation has achieved higher macro-average precision results compared to our Empirical Bayes experiment. Using unigrams to perform multi-class classification, we had macro-average precisions 0.19, 0.24 using past tweets and sample tweets, respectively. But when we investigated the predictions, we have seen that the classifier was almost always predicting the location with highest amount of tweets(Manhattan, NY).

\subsubsection{Normalisation of Documents per Class}
To prevent the problem we faced in MultinomialNB test, we have normalised the dataset number of documents per class. To do so, we normalized the number of documents per class by limiting the \textit{maximum number of documents per class} to the \textit{class with least amount of documents}. That reduced the elements in our training dataset to \textit{number of classes}(8) times \textit{number of documents in class with least documents}(e.g. \textit{Atlanta, GA} with 122 samples)  for multi-class configuration. For binary classification, we have used the class with maximum number of documents(e.g. \textit{Manhattan, NY} with 976 samples), and combined it with same amount of random samples for non-class training data. Upon normalisation, our multi-class training dataset shrank considerably, but we have achieved macro-average precision rates of 0.29, 0.42 for past tweets and sample tweets, using unigrams.

\subsection{Scikit Learn - Algorithm Comparison}
To see which approach is best for our research question and what kind of token based textual features are useful for our task, we compared some classification methods implemented in Scikit Learn\footnote{http://scikit-learn.org/stable/supervised\_learning.html}. Scikit Learn implements various classification methods such as, Logistic Regression, Perceptron, Support Vector Classifiers and etc. 

Scikit Learn implements different model configurations. A good example is, the loss function of SGDClassifier. There are 9 different loss functions implemented that we could use with SGDClassifier. In our experiment, we tried to capture some of these different parameters, and tried to explain them. But in most cases, we didn't try to fine tune the hyper-parameters.

\begin{enumerate}
\scriptsize

\item LogisticRegressionCV: Uses a logistic function that combines the tf-idf scores with weights in a logistic function. This model is useful if our tf-idf scores have exponential effects.
\item SGDClassifier: Stochastic Gradient Descent creates a linear function and gives initial weights to each token for each class. Based on the loss functions defined below, it tries to optimize these weights.
\begin{enumerate}
\item SGDClassifier(loss="hinge"): Hinge is a commonly used linear loss function for classification.
\item SGDClassifier(loss="log"): Converts SGD to a probabilistic logistic regression classifier.
\item SGDClassifier(loss="modified\_huber"): Hinge with a setting, $\gamma=2$
\item SGDClassifier(loss="squared\_hinge"): Quadratically penalised hinge.
\item SGDClassifier(loss="perceptron"): Linear loss function used with the Perceptron algorithm.
\item SGDClassifier(loss="squared\_loss"): Least squares loss function.
\item SGDClassifier(loss="huber"): Similar to squared\_loss but switches from squared to linear after a threshold.
\item SGDClassifier(loss="epsilon\_insensitive"): Linear loss function, ignoring errors less than the threshold, epsilon.
\item SGDClassifier(loss="squared\_epsilon\_insensitive"): Squared version of epsilon\_insensitive.
\end{enumerate}
\item Perceptron: Same as SGDClassifier(loss="perceptron"), updates the tf-idf weights with a constant learning rate and has different default parameters.
\item PassiveAggressiveClassifier: Similar to Perceptron. Instead of using a learning rate, uses a regularisation term that makes sure that tf-idf weights don't get too big and prevents overfitting.
\begin{enumerate}
\item PassiveAggressiveClassifier(loss="hinge")
\item PassiveAggressiveClassifier(loss="squared\_hinge")
\end{enumerate}
\item SVC: Classifier model using Support Vector Machines. Converts tf-idf scores based on the kernel function to create a high or infinite dimensional hyperplanes by clustering classes on that space.
\begin{enumerate}
\item SVC(kernel='poly'): Polynomial kernel for SVM.
\item SVC(kernel='sigmoid'): Sigmoid kernel for SVM.
\end{enumerate}
\item LinearSVC: SVM with a linear kernel.
\item KNeighborsClassifier: K-Nearest Neighbours classifier, uses weights function to cluster and classify tf-idf scores using the function given in weights. Finds the $n_neighbours$ number of tweets with most similar tokens from training dataset, casts a vote among them make predictions.
\begin{enumerate}
\item KNeighborsClassifier(n\_neighbors=8, weights="uniform"): Uses uniform weights to classify neighbours.
\item KNeighborsClassifier(n\_neighbors=8, weights="distance"): Inverse of distance weights. Closer neighbours have a greater influence on classification.
\end{enumerate}
\item MultinomialNB: Gives probabilities for each token and class, combines them to find the most likely class.
\item BernoulliNB: Similar to Multinomial, but instead of using tf-idf scores, considers if a token occurs in a tweet or not.
\item DecisionTreeClassifier: Creates a rule based decision tree inferred from data for predict.
\item AdaBoostClassifier(n\_estimators=170): Fits N($n\_estimators$) Decision Tree Classifiers to samples from training data. Then assigns them weights based on errors. Using those weights, casts a voting to find the best prediction.
\end{enumerate}
\normalsize
We have tried those 22 models, with 8 configurations to create 176 different combinations.

\subsubsection{Execution}

To execute those tasks, we have used Google Cloud instances. Google Cloud offers 300\$ trial usage for users who want to try out their services and it allows up to 32 cores in this trial period. We have implemented an argument to execute all those tasks on multiple google cloud instances, in parallel. Created a template operating system including an executable version of our implementation. Using 15 two core execution instances from the execution template, 1 MongoDB instance, it took 35 mins to finish the execution of those 176 tasks. 

\section{Results}

\subsection{Binary Classification Baselines}
We have considered 2 baselines to compare our results for predicting \textit{Manhattan, NY}. First one is making random choices with a baseline macro-average precision/recall and f1-score of 0.5.

The second baseline is through choosing tweets containing important keywords. To do so, we have created a regular expression and ran it through out dataset. This regular expression created a baseline macro-average precision of 0.8. Compared to the high macro-average precision, it had a very low recall 0.37 for predicting our location, \textit{Manhattan, NY}. Since we are interested in being able to say if a tweet is sent within a location, we're not going to use this baseline for comparison.

\begin{verbatim}
.*(([Mm]anhattan)|([Nn]ew[ ]?[Yy]ork)|(NY)).*
\end{verbatim} 


Combining the random baseline and regular expression, we have created a new baseline. In this baseline, if the regular expression matches we classify this data as \textit{Manhattan, NY}, otherwise we make random choices. This combined baseline gives us a 0.68 recall, 0.57 precision and 0.62 f1-score for \textit{Manhattan, NY} with 0.59 macro-averaged recall, 0.59 macro-averaged precision and 0.59 macro-averaged f1-score. 

\subsection{Binary Classification}

Including bigrams, BernoulliNB, SGD with modified huber loss function and SGD with hinge loss function have achieved the top 3 macro-averaged f1scores of 0.734, 0.726, 0.72, respectively. 

Only with unigrams BernoulliNB, MultinomialNB and PassiveAgressiveClassifier with hinge loss function achieved the top 3 macro-averaged f1scores of 0.726, 0.719 and 0.717, respectively. 

\subsection{Multi-Class Classification}
Random baseline compared with location names(as in the for 8 locations and

\subsection{Assumption of Past Tweets}
Using past tweets as our dataset reduced the classifier performance of almost all configurations. Overall average decrease in f1scores was 0.076 with values ranging from 0.21 to -0.09. Our assumption was wrong.

\subsection{Textual Features}

When we investigated why BernoulliNB is achieving the highest performance, we have seen that in most of the cases(around 90\% of the correct predictions), there was a building or place name included in the tokens(e.g. \textit{Central Park}, \textit{Empire State}). In the rest of the cases(around 10\%), there were no obvious textual features. In those cases we assumed that there was a unique token that only appeared in tweets labelled as \textit{Manhattan, NY}. BernoulliNB is the simplest of all, it only considers if a token appears in a location or not.


\section{Conclusion}

In our research question, we asked if there are any textual features that are shared by the people who are within a high-level location. In our experiments, we couldn't find any specific linguistic or textual features in English that hints to the geolocation(i.e. \textit{Manhattan, NY}) of a tweet, except the location names.

Using the location names included in tweets, were able to improve the combined baseline(macro-averaged f1score) for binary classification around 0.16.

\section{Discussion}

The fact of all our data was geotagged may be adding a bias to our dataset and compromising the validity of our results. It may be possible for geotagged tweets to include more location names. Refining the training dataset with more user generated content(i.e. removing Foursquare or Yelp from source whitelist) and collecting more data would yield more authentic results and more textual features.

We have seen that BernoulliNB had one of the top results in binary classification. This hints to having tf-idf scores may not be a good idea, because this method ignores these scores and uses binary values. Instead of having tf-idf scores, having binary values in the sparse array and trying different configurations on this dataset might yield to a better improvement on the baseline. But as we have concluded, we couldn't see any unique words that identify the high-level location of user.

To improve Empirical Bayes, we have ignored tokens that are not appearing in a location. This solution was wrong because it sets the likelihood to 1 when it can't find a token on a location. A solution would be setting the likelihood as if that token appeared only once.
\bibliographystyle{alpha}
\bibliography{paper}
\end{document}




























